{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA Model Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install tflearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "from pandas import Series\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "import numpy as np\n",
    "import time, datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import string\n",
    "from collections import Counter\n",
    "from nltk import ngrams\n",
    "import itertools\n",
    "import random\n",
    "from matplotlib.collections import LineCollection\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification  \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.metrics import roc_curve  \n",
    "from sklearn.metrics import roc_auc_score  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the data, converting the time to timestamp and indexing the date to use it as a time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Paths\n",
    "DATA_PATH = sys.path[0]+\"\\\\data\\\\\"\n",
    "filename_train1=\"BATADAL_dataset03.csv\"\n",
    "filename_train2=\"BATADAL_dataset04.csv\"\n",
    "filename_test=\"BATADAL_test_dataset.csv\"\n",
    "\n",
    "#Reading the data\n",
    "dftrain1  = pd.read_csv(DATA_PATH + filename_train1)\n",
    "dftrain2  = pd.read_csv(DATA_PATH + filename_train2)\n",
    "dftest  = pd.read_csv(DATA_PATH + filename_test)\n",
    "\n",
    "# Modify string date to timestamp\n",
    "dftrain1.DATETIME = dftrain1.DATETIME.apply(lambda s: pd.to_datetime(s,format='%d/%m/%y %H'))\n",
    "dftrain2.DATETIME = dftrain2.DATETIME.apply(lambda s: pd.to_datetime(s,format='%d/%m/%y %H'))\n",
    "dftest.DATETIME = dftest.DATETIME.apply(lambda s: pd.to_datetime(s,format='%d/%m/%y %H'))\n",
    "\n",
    "# Choosing 0 if not an anomaly and 1 if anomaly\n",
    "dftrain2[' ATT_FLAG']=dftrain2[' ATT_FLAG'].apply(lambda x: 0 if x==-999 else x)\n",
    "\n",
    "# Indexing\n",
    "dftrain1=dftrain1.set_index('DATETIME')\n",
    "dftrain2=dftrain2.set_index('DATETIME')\n",
    "dftest=dftest.set_index('DATETIME')\n",
    "\n",
    "#dftrain1.shift(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Creating Functions to Evaluate Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Observation: some of the code used here was derived from the website: This method was derived from: https://www.oreilly.com/library/view/hands-on-unsupervised-learning/9781492035633/ch04.html that performed a similar analisis using PCA_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transactions that have the largest sum of squared differences will have an error close to one, while those that have the smallest sum of squared differences will have an error close to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anomalyScores(originalDF, reducedDF):\n",
    "    #Calculating the square loss\n",
    "    loss = np.sum((np.array(originalDF)-np.array(reducedDF))**2, axis=1)\n",
    "    #Changing format\n",
    "    loss = pd.Series(data=loss,index=originalDF.index)\n",
    "    #Scaling between 0 and 1\n",
    "    loss = (loss-np.min(loss))/(np.max(loss)-np.min(loss))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot anomaly scores and performance metrics. Return predictions if the user wants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotResults(trueLabels, anomalyScores, returnPreds = False):\n",
    "    preds = pd.concat([trueLabels, anomalyScores], axis=1)\n",
    "    preds.columns = ['trueLabel', 'anomalyScore']\n",
    "    precision, recall, thresholds = \\\n",
    "        precision_recall_curve(preds['trueLabel'],preds['anomalyScore'])\n",
    "    average_precision = \\\n",
    "        average_precision_score(preds['trueLabel'],preds['anomalyScore'])\n",
    "\n",
    "    plt.step(recall, precision, color='k', alpha=0.7, where='post')\n",
    "    plt.fill_between(recall, precision, step='post', alpha=0.3, color='k')\n",
    "\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "\n",
    "    plt.title('Precision-Recall curve: Average Precision = \\\n",
    "    {0:0.2f}'.format(average_precision))\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(preds['trueLabel'], \\\n",
    "                                     preds['anomalyScore'])\n",
    "    areaUnderROC = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='r', lw=2, label='ROC curve')\n",
    "    plt.plot([0, 1], [0, 1], color='k', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic: \\\n",
    "    Area under the curve = {0:0.2f}'.format(areaUnderROC))\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    if returnPreds==True:\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting a scatterplot to visualize the predicitons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatterPlot(xDF, yDF, algoName):\n",
    "    tempDF = pd.DataFrame(data=xDF.loc[:,0:1], index=xDF.index)\n",
    "    tempDF = pd.concat((tempDF,yDF), axis=1, join=\"inner\")\n",
    "    tempDF.columns = [\"First Vector\", \"Second Vector\", \"Label\"]\n",
    "    sns.lmplot(x=\"First Vector\", y=\"Second Vector\", hue=\"Label\", \\\n",
    "               data=tempDF, fit_reg=False)\n",
    "    ax = plt.gca()\n",
    "    ax.set_title(\"Separation of Observations using \"+algoName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30 principal components\n",
    "n_components = 30\n",
    "whiten = False\n",
    "random_state = 2018\n",
    "\n",
    "pca = PCA(n_components=n_components, whiten=whiten, \\\n",
    "          random_state=random_state)\n",
    "\n",
    "X_train_PCA = pca.fit_transform(X_train)\n",
    "X_train_PCA = pd.DataFrame(data=X_train_PCA, index=X_train.index)\n",
    "\n",
    "X_train_PCA_inverse = pca.inverse_transform(X_train_PCA)\n",
    "X_train_PCA_inverse = pd.DataFrame(data=X_train_PCA_inverse, \\\n",
    "                                   index=X_train.index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ploting the scatterplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatterPlot(X_train_PCA, y_train, \"PCA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ploting the anomaly scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalyScoresPCA = anomalyScores(X_train, X_train_PCA_inverse)\n",
    "preds = plotResults(y_train, anomalyScoresPCA, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.sort_values(by=\"anomalyScore\",ascending=False,inplace=True)\n",
    "cutoff = 350\n",
    "predsTop = preds[:cutoff]\n",
    "print(\"Precision: \",np.round(predsTop. \\\n",
    "            anomalyScore[predsTop.trueLabel==1].count()/cutoff,2))\n",
    "print(\"Recall: \",np.round(predsTop. \\\n",
    "            anomalyScore[predsTop.trueLabel==1].count()/y_train.sum(),2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
